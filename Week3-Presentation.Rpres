Floodplain Forest Group: Progress Report, Week 3
========================================================
author: Sal Balkus, Noah Dean, Makayla McDevitt 
date: 6/19/20
autosize: true
css: Week3-Presentation.css
type: section

```{r, echo = FALSE}

#knitr::opts_chunk$set(warning = F, error = F, message = F, echo = F, include = F)
#knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
opts_chunk$set(cache=TRUE)


library(tidyverse)
library(cluster)
library(dbscan)

dissim <- read_csv("dissimilarity_matrix.csv")
dissim <- as.matrix(dissim)

#df <- read_csv('clean_data/UMRS_FPF_clean.csv')
#plots <- read_csv("clean_data/plots_classification.csv")

library(tidyverse)
library(kernlab)
library(dbscan)
library(vegclust)
library(cluster)

df <- read_csv("clean_data/UMRS_FPF_clean.csv")
labels <- read_csv("clean_data/plot_classification.csv")

df_cols <- left_join(df, labels, by = "PID") %>% select(PID, TR_SP, BasalArea, TreesPerAcre, Type, Label, TR_DIA)
df_acsa2 <- filter(df_cols, Type == "ACSA2")
```

Classification Overview 
========================================================

Goal: Classify UMRS floodplain forests in terms of composition and structure 

Two levels of classification: 

1. Tree species dominance

  -Density 
  
  -Basal area 
  
2. Clustering 


========================================================
![](week_3_pres_images/classification .png)


========================================================
![](week_3_pres_images/single-species.png)

***


![](week_3_pres_images/dominant.png) 

![](week_3_pres_images/dominant_number.png)


========================================================
![](week_3_pres_images/codominant_photo .png)

*** 

![](week_3_pres_images/codominant .png)
![](week_3_pres_images/codominant_number.png)

========================================================
![](week_3_pres_images/mixed_photo.png)

*** 

![](week_3_pres_images/mixed.png) 
![](week_3_pres_images/Mixed_number.png)


What is CAP
========================================================
- Cumulative abundance profile

  - Total amount of trees in or above a size class

- Uses the distribution of sizes within a species

- Allows for exploration of variation with same-species plots

CAP Example
=====
![](week_3_pres_images/graphs.png)

(DeCaceres et al, 2013)

Why care about the size distribution?
======
- The size distribution will affect how the forest behaves

- External processes may have different impacts

- Time to restore

- Allows for more efficient use of management resources

Size distribution example
====
![Emerald ash borer](week_3_pres_images/eab.jpg)

Emerald ash borer (Arbor day foundation)

***
![Ash tree](week_3_pres_images/ash.jpg)

Ash tree (Arbor day foundation)


How are plots compared?
=====
- Uses 3 metrics

![3 equations](week_3_pres_images/eq1.png)

- Bray-Curtis dissimilarity coefficient:

![Dissimilarity equation](week_3_pres_images/eq2.png)

(DeCaceres et al, 2013)


Example of metrics
====
![Examples of metrics](week_3_pres_images/examples.png)

(DeCaceres et al, 2013)


Our plots
====
```{r echo = F, include = F}

# unique(df_acsa2$PID)

# df_acsa2 %>% count(PID, sort = T)

example_df <- df_acsa2 %>% filter(PID %in% c('GILBERT-2-96', 'COTTONWOOD-1-2'))

DIA_bins <- 1:70

test <- stratifyvegdata(example_df, sizes1 = DIA_bins, plotColumn = "PID", speciesColumn = "TR_SP", abundanceColumn = "TreesPerAcre", size1Column = "TR_DIA" )
cap <- CAP(test)

# test

plot.CAP(cap[1], xlab = 'Tree diameter', ylab = 'Trees per acre', main = 'CAP of two plots', col = 'black')
plot.CAP(cap[2], xlab = 'Tree diameter', ylab = 'Trees per acre', col = 'red', add = T)
legend('topright', legend = c('ACSA2', 'SNAG'), lty = c('solid', 'dashed'))
legend(51, 190, legend = c('COTTONWOOD-1-2','GILBERT-2-96'), fill = c('black', 'red'))

dissim <- vegdiststruct(cap, method = "bray")
dissim
```
![Our plots](week_3_pres_images/Rplot.png)

***
- The distance between them is `r round(dissim[1], digits = 4)`

Level 2 Classification
========================================================

Our next goal is to subdivide the Level 1 categories using clustering.

The number of clusters should be numerous enough to capture different forest types within the Level 1 categories, but not so numerous that similar forest types are repeated across multiple clusters.



Strategy
========================================================
Because the Level 1 categories are so numerous, a systematic approach must be developed to generate subcategories.

First, we perform experimentation using the ACSA2-dominant (silver maple) plots. Through this, we develop a function to select the appropriate number of clusters. Since "silver maple dominant" is the most numerous and complex (besides mixed), our approach developed here will not be too simplistic for any other group.

Once our function is developed, we will apply it across all level 1 classifications. Mixed plots will be clustered separately, since they are the largest level 1 category, much larger than others.



Potential Clustering Methods
========================================================
Level 2 categories are determined via clustering, which groups plots based on their dissimilarity (Bray-Curtis, based on CAP values).

We considered several potential clustering algorithms:
- K-means
- Hierarchical (single linkage, complete linkage, Ward's method)
- DBSCAN/OPTICS
- Spectral Clustering

These clustering algorithms were each tested on our data to determine their effectiveness


Spectral Clustering
========================================================
A graph-based clustering algorithm especially good for high-dimensional data
- Uses graph Laplacian eigenvalues to partition the data points
- Performs dimension reduction
- Good at picking out unique shapes
-O(n^3)

We discussed using this algorithm to cluster the data without using CAP. However, our data was too large for the slow algorithm, and the CAP values solved the high-dimensionality problem.

=======
http://people.csail.mit.edu/dsontag/courses/ml14/notes/Luxburg07_tutorial_spectral_clustering.pdf


DBSCAN & OPTICS
========================================================
Algorithms that group observations based on density
- DBSCAN: specify minimum distance and minimum observations in each cluster
- OPTICS: specify minimum observations per cluster; creates a dendrogram that can be cut
- Can mark points as outliers if they do not fit a cluster
- No need to specify number of clusters!

https://medium.com/@xzz201920/optics-d80b41fd042a#:~:text=Reachability%2Dplot%20to%20Clustering&text=It%20is%20a%202D%20plot,valleys%20in%20the%20reachability%20plot.




OPTICS: Reachability plot
========================================================

The deeper the valley, the denser the cluster.

```{r, echo = FALSE}
cluster_o <- optics(as.matrix(dissim), eps = 10, minPts = 6)

cluster_db <- extractDBSCAN(cluster_o, eps_cl = 2)
plot(cluster_db)

cluster_count <- length(unique(cluster_db$cluster))
cat("Number of clusters from DBSCAN:", cluster_count)

```

***

```{r, echo = FALSE}

cluster_db <- extractDBSCAN(cluster_o, eps_cl = 1)
plot(cluster_db)

cluster_count <- length(unique(cluster_db$cluster))
cat("Number of clusters from DBSCAN:", cluster_count)
```

K-means
========================================================
Clustering method that performs partitioning by optimizing centroid placement
- assumes clusters to be spherical
- assumes equal variance within clusters


K-means
========================================================

```{r, echo = FALSE}
#Evaluate different numbers of clusters for kmeans
clusters <- 20
cluster_k <- vector("list", length = clusters)
for(n in 1:clusters) {cluster_k[[n]] <- kmeans(dissim, n)}

#Elbow method
cluster_k_twss <- vector(length = clusters)
for(n in 1:clusters){
  cluster_k_twss[n] <- cluster_k[[n]]$tot.withinss
}

plt_df <- as.data.frame(list(cluster_k_twss, seq(1:clusters)), row.names = NULL, col.names = c("TWSS","Index"))
ggplot(data = plt_df, mapping = aes(x = Index, y = TWSS)) + geom_line(color = "lightblue") + geom_point() + theme_light() +
  xlab("Number of Clusters") + ylab("Total within-cluster sum of squares")

```

K-means
========================================================

```{r, echo = FALSE}
#Silhouette
sil <- vector("list", length = clusters)
for(n in 1:clusters){sil[[n]] <- silhouette(x = cluster_k[[n]]$cluster, dmatrix = as.matrix(dissim))}
plot(sil[[3]], col = c("red","blue","black"), border = NA)
```

***

```{r, echo = FALSE}
#Plot average sil length
avg_sil_len <- vector(length = clusters-1)
min_sil_len <- vector(length = clusters-1)
avg_sil_neg <- vector(length = clusters-1)
for(n in 2:clusters){avg_sil_len[n-1] <- mean(sil[[n]][,3])}
for(n in 2:clusters){min_sil_len[n-1] <- min(sil[[n]][,3])}
for(n in 2:clusters){avg_sil_neg[n-1] <- mean(sil[[n]][sil[[n]][,3] < 0,3])}

plt_df <- as.data.frame(list(avg_sil_len, min_sil_len, avg_sil_neg, seq(2:clusters)), col.names = c("avg","min","avgneg","Index"))
ggplot(data = plt_df) + geom_point(aes(x = Index, y = avg, color = "Average Silhouette Length")) + geom_line(aes(x = Index, y = avg, color = "Average Silhouette Length")) + 
  geom_point(aes(x = Index, y = min, color = "Minimum Silhouette Length")) + geom_line(aes(x = Index, y = min, color = "Minimum Silhouette Length")) + 
  geom_point(aes(x = Index, y = avgneg, color = "Average Negative Silhouette Length")) + geom_line(aes(x = Index, y = avgneg, color = "Average Negative Silhouette Length")) + theme_light() + ylab("Silhouette Length") + xlab("Number of Clusters") + theme(legend.title = element_blank())
```

K-means
========================================================
```{r, echo = FALSE}
gap <- read_csv("gap_kmeans.csv")
gap$Index <- seq(1:20)
ggplot(gap,aes(x = Index, y = gap)) + geom_line(color = "lightblue") + geom_point() + theme_light()

selected_value <- maxSE(gap$gap, gap$SE.sim, method = "Tibs2001SEmax")
cat("Recommended clusters:", selected_value)

```

Hierarchical Clustering
========================================================
Joins points based on closeness to create a dendrogram
- Single-linkage agglomerative: joins clusters based on closest point
  - Good at differentiating complex shapes with clear boundaries
  - Without clear boundaries, often creates #Hierarchical Clustering
- Complete-linkage agglomerative: joins clusters based on farthest point
- Ward's method: joins clusters based on minimizing within-cluster variance

Single Linkage
========================================================
Unsuitable, yields an output similar to OPTICS

The single linkage picks out too many outliers, preventing the clusters from being split into actual groups

***

```{r, echo = FALSE}
cluster_h <- hclust(as.dist(dissim), method = "single")
plot(cluster_h, labels = FALSE, xlab = "Forest Plot")
```

Complete Linkage
========================================================

Still problematic - no clear cut point

Clusters are too close together, indicating that there is no real difference between the clusters

***

```{r, echo = FALSE}
cluster_h <- hclust(as.dist(dissim), method = "complete")
plot(cluster_h, labels = FALSE, xlab = "Forest Plot")
```

Ward's Method (ward.D)
========================================================

Best approach thus far; clusters are appropriately distanced, and split into roughly even-sized groups.

Still need to investigate where to cut the dendrogram, and how to validate this clustering solution.

***

```{r, echo = FALSE}
cluster_h <- hclust(as.dist(dissim), method = "ward.D")
plot(cluster_h, labels = FALSE, xlab = "Forest Plot")
```


Next Steps
========================================================

Endnotes
========================================================

Cover Image: Forest Landscape Ecology of the Upper Mississippi River Floodplain, United States Geological Survey

